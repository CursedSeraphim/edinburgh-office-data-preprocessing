{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9ba7d9",
   "metadata": {},
   "source": [
    "## Edinburgh CCTV CNN Autoencoder (2D Latent Space)\n",
    " \n",
    "Trains a convolutional autoencoder on the pixel data from the\n",
    "trajectories CSV generated in the first notebook.\n",
    "\n",
    "- Input: CSV with pixel columns (1x1, 1x2, ..., WxH or *_r/g/b)\n",
    "- Output: trained CNN autoencoder with 2D latent space\n",
    "- Config: all architecture and training hyperparameters are read from a YAML file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e533460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: install dependencies in a fresh environment\n",
    "# # Uncomment as needed.\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install pyyaml pandas numpy matplotlib scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import tarfile\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "\n",
    "from PIL import Image\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Filename pattern for Edinburgh frames (day_*.tar contents)\n",
    "FILENAME_RE = re.compile(\n",
    "    r\"inspacecam163_(\\d{4})_(\\d{2})_(\\d{2})_(\\d{2})_(\\d{2})_(\\d{2})\\.jpg$\"\n",
    ")\n",
    "\n",
    "# Make matplotlib a bit nicer\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf76c9",
   "metadata": {},
   "source": [
    "## 1. Load YAML configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Path to the YAML configuration file\n",
    "# Adjust as needed\n",
    "CONFIG_PATH = \"configs/ae_cnn_example.yaml\"\n",
    "\n",
    "if not os.path.exists(CONFIG_PATH):\n",
    "    os.makedirs(os.path.dirname(CONFIG_PATH), exist_ok=True)\n",
    "    example_cfg = \"\"\"\n",
    "seed: 0\n",
    "\n",
    "data:\n",
    "  # Folder containing day_*.tar files from the Edinburgh office dataset\n",
    "  folder: \".\"\n",
    "  grayscale: true              # true = convert to 1 channel, false = RGB (3 channels)\n",
    "  use_all_times: false         # if true, ignore start_time/end_time\n",
    "  start_time: \"15:00:00\"\n",
    "  end_time: \"15:59:59\"\n",
    "  frame_step: 10               # keep every N-th frame per day (after time filtering & sorting)\n",
    "  max_samples: null            # null = use all selected frames, int = random subset\n",
    "  train_fraction: 0.9          # fraction of samples used for training, rest for validation\n",
    "  normalize: true              # divide pixel values by 255.0\n",
    "\n",
    "model:\n",
    "  in_channels: 1               # 1 for grayscale, 3 for RGB\n",
    "  img_width: 128\n",
    "  img_height: 72\n",
    "  conv_channels: [32, 64, 128] # encoder conv channels per stage\n",
    "  kernel_size: 3\n",
    "  latent_dim: 2\n",
    "  dropout: 0.1\n",
    "\n",
    "training:\n",
    "  num_epochs: 50\n",
    "  batch_size: 256\n",
    "  optimizer: adam              # \"adam\" or \"sgd\"\n",
    "  learning_rate: 1.0e-3\n",
    "  weight_decay: 1.0e-5\n",
    "\n",
    "device:\n",
    "  type: \"auto\"                 # \"auto\", \"cuda\", or \"cpu\"\n",
    "\n",
    "logging:\n",
    "  output_dir: \"autoencoder_runs\"\n",
    "  run_name: \"cnn_ae_v1\"\n",
    "  num_recon_examples: 8\n",
    "\"\"\"\n",
    "    with open(CONFIG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(example_cfg.strip() + \"\\n\")\n",
    "    print(f\"Wrote example config to {CONFIG_PATH}\")\n",
    "\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    config: Dict[str, Any] = yaml.safe_load(f)\n",
    "\n",
    "print(\"Loaded config from:\", CONFIG_PATH)\n",
    "config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d9286",
   "metadata": {},
   "source": [
    "## 2. Reproducibility helpers and column detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a9aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def set_global_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "\n",
    "\n",
    "set_global_seed(config.get(\"seed\", 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36152e56",
   "metadata": {},
   "source": [
    "## 3. Index frames directly from day\\_*.tar files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def parse_time_from_filename(name: str):\n",
    "    \"\"\"Returns (hour, minute, second) or None if the filename does not match.\"\"\"\n",
    "    base = os.path.basename(name)\n",
    "    m = FILENAME_RE.match(base)\n",
    "    if not m:\n",
    "        return None\n",
    "    h = int(m.group(4))\n",
    "    m_ = int(m.group(5))\n",
    "    s = int(m.group(6))\n",
    "    return h, m_, s\n",
    "\n",
    "\n",
    "def seconds_since_midnight(h: int, m: int, s: int) -> int:\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "\n",
    "def parse_time_string(s: str) -> int:\n",
    "    \"\"\"\n",
    "    Parse HH:MM or HH:MM:SS -> seconds since midnight.\n",
    "    \"\"\"\n",
    "    parts = s.split(\":\")\n",
    "    if len(parts) == 2:\n",
    "        h, m = parts\n",
    "        s_val = 0\n",
    "    elif len(parts) == 3:\n",
    "        h, m, s_val = parts\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid time format: {s!r}. Expected HH:MM or HH:MM:SS\")\n",
    "    h_i = int(h)\n",
    "    m_i = int(m)\n",
    "    s_i = int(s_val)\n",
    "    if not (0 <= h_i < 24 and 0 <= m_i < 60 and 0 <= s_i < 60):\n",
    "        raise ValueError(f\"Time out of range: {s!r}\")\n",
    "    return seconds_since_midnight(h_i, m_i, s_i)\n",
    "\n",
    "\n",
    "def time_filter(day_sec: int, start_sec: int, end_sec: int, use_all_times: bool) -> bool:\n",
    "    if use_all_times:\n",
    "        return True\n",
    "    return start_sec <= day_sec <= end_sec\n",
    "\n",
    "\n",
    "def list_tar_files(folder: str) -> List[str]:\n",
    "    pattern = os.path.join(folder, \"day_*.tar\")\n",
    "    tar_paths = sorted(glob.glob(pattern))\n",
    "    if not tar_paths:\n",
    "        raise SystemExit(f\"No tar files matching 'day_*.tar' found in {folder}\")\n",
    "    return tar_paths\n",
    "\n",
    "\n",
    "data_cfg = config[\"data\"]\n",
    "model_cfg = config[\"model\"]\n",
    "\n",
    "data_folder = data_cfg[\"folder\"]\n",
    "grayscale = bool(data_cfg.get(\"grayscale\", True))\n",
    "use_all_times = bool(data_cfg.get(\"use_all_times\", True))\n",
    "start_time = data_cfg.get(\"start_time\", \"00:00:00\")\n",
    "end_time = data_cfg.get(\"end_time\", \"23:59:59\")\n",
    "frame_step = int(data_cfg.get(\"frame_step\", 1))\n",
    "max_samples = data_cfg.get(\"max_samples\", None)\n",
    "\n",
    "img_w = int(model_cfg[\"img_width\"])\n",
    "img_h = int(model_cfg[\"img_height\"])\n",
    "in_channels_model = int(model_cfg[\"in_channels\"])\n",
    "in_channels_data = 1 if grayscale else 3\n",
    "if in_channels_model != in_channels_data:\n",
    "    raise SystemExit(\n",
    "        f\"model.in_channels={in_channels_model} does not match data.grayscale \"\n",
    "        f\"(expected {in_channels_data} channels).\"\n",
    "    )\n",
    "in_channels = in_channels_model\n",
    "\n",
    "if frame_step <= 0:\n",
    "    raise ValueError(\"data.frame_step must be >= 1\")\n",
    "\n",
    "if use_all_times:\n",
    "    start_sec = 0\n",
    "    end_sec = 24 * 3600 - 1\n",
    "else:\n",
    "    start_sec = parse_time_string(start_time)\n",
    "    end_sec = parse_time_string(end_time)\n",
    "    if end_sec < start_sec:\n",
    "        raise ValueError(\"end_time must be >= start_time\")\n",
    "\n",
    "tar_paths = list_tar_files(data_folder)\n",
    "print(f\"Found {len(tar_paths)} tar files in {data_folder}\")\n",
    "\n",
    "frame_records: List[Dict[str, Any]] = []\n",
    "line_index_by_path: Dict[str, int] = {}\n",
    "line_counter = 0\n",
    "\n",
    "# --- tqdm added here ---\n",
    "for path in tqdm(tar_paths, desc=\"Scanning tar files\"):\n",
    "    line_id = line_counter\n",
    "    line_index_by_path[path] = line_id\n",
    "    line_counter += 1\n",
    "\n",
    "    with tarfile.open(path, \"r\") as tar:\n",
    "        # Wrap the inner loop too\n",
    "        members = [m for m in tar.getmembers() if m.isfile()]\n",
    "\n",
    "        frames = []\n",
    "        for member in tqdm(members, leave=False, desc=f\"Reading {os.path.basename(path)}\"):\n",
    "            t = parse_time_from_filename(member.name)\n",
    "            if t is None:\n",
    "                continue\n",
    "            h, m_, s = t\n",
    "            day_sec = seconds_since_midnight(h, m_, s)\n",
    "            if not time_filter(day_sec, start_sec, end_sec, use_all_times):\n",
    "                continue\n",
    "            time_str = f\"{h:02d}:{m_:02d}:{s:02d}\"\n",
    "            frames.append((day_sec, time_str, member.name))\n",
    "\n",
    "        # Sort in time and apply per-day frame_step\n",
    "        frames.sort(key=lambda x: x[0])\n",
    "        frames = frames[::frame_step]\n",
    "\n",
    "        for step, (day_sec, time_str, member_name) in enumerate(frames):\n",
    "            frame_records.append(\n",
    "                {\n",
    "                    \"tar_path\": path,\n",
    "                    \"member_name\": member_name,\n",
    "                    \"line\": line_id,\n",
    "                    \"step\": step,\n",
    "                    \"day_sec\": day_sec,\n",
    "                    \"time_str\": time_str,\n",
    "                }\n",
    "            )\n",
    "\n",
    "if not frame_records:\n",
    "    raise SystemExit(\"No frames found in the specified time window.\")\n",
    "\n",
    "print(f\"Total selected frames before sampling: {len(frame_records)}\")\n",
    "\n",
    "# --- tqdm for subsampling ---\n",
    "if max_samples is not None:\n",
    "    max_samples = int(max_samples)\n",
    "    if max_samples > 0 and max_samples < len(frame_records):\n",
    "        rng = np.random.default_rng(config.get(\"seed\", 0))\n",
    "        idx = rng.choice(len(frame_records), size=max_samples, replace=False)\n",
    "        idx_sorted = np.sort(idx)\n",
    "        frame_records = [frame_records[i] for i in tqdm(idx_sorted, desc=\"Applying max_samples\")]\n",
    "        print(f\"Subsampled to {len(frame_records)} frames based on max_samples={max_samples}\")\n",
    "\n",
    "line_col_present = True\n",
    "\n",
    "print(\"Distinct trajectories (days):\", len(line_index_by_path))\n",
    "print(\"Final number of frames:\", len(frame_records))\n",
    "print(\"Image dimensions:\", img_w, \"x\", img_h, \"channels =\", in_channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68854fa",
   "metadata": {},
   "source": [
    "## 4. Dataset and DataLoader (directly from tar files)\n",
    " \n",
    "Each row is one frame from day\\_*.tar, loaded and downsampled on the fly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "_tar_cache: Dict[str, tarfile.TarFile] = {}\n",
    "\n",
    "\n",
    "def _get_tar_handle(path: str) -> tarfile.TarFile:\n",
    "    tf = _tar_cache.get(path)\n",
    "    if tf is None:\n",
    "        tf = tarfile.open(path, \"r\")\n",
    "        _tar_cache[path] = tf\n",
    "    return tf\n",
    "\n",
    "\n",
    "class TrajectoryFrameDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that reads images directly from day_*.tar files and converts them\n",
    "    into (C, H, W) tensors.\n",
    "\n",
    "    Each sample corresponds to one frame specified in `frame_records`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        frame_records: List[Dict[str, Any]],\n",
    "        img_width: int,\n",
    "        img_height: int,\n",
    "        in_channels: int = 1,\n",
    "        grayscale: bool = True,\n",
    "        normalize: bool = True,\n",
    "        return_line: bool = True,\n",
    "    ):\n",
    "        # Avoid copying a huge list here; we just keep the reference.\n",
    "        # If you want a progress bar, do it once earlier when building frame_records.\n",
    "        self.records = frame_records\n",
    "        self.img_w = img_width\n",
    "        self.img_h = img_height\n",
    "        self.in_channels = in_channels\n",
    "        self.grayscale = grayscale\n",
    "        self.normalize = normalize\n",
    "        self.return_line = return_line\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def _load_image_tensor(self, rec: Dict[str, Any]) -> torch.Tensor:\n",
    "        tf = _get_tar_handle(rec[\"tar_path\"])\n",
    "        member = tf.getmember(rec[\"member_name\"])\n",
    "        fileobj = tf.extractfile(member)\n",
    "        if fileobj is None:\n",
    "            raise RuntimeError(\n",
    "                f\"Could not extract {rec['member_name']} from {rec['tar_path']}\"\n",
    "            )\n",
    "\n",
    "        img = Image.open(fileobj)\n",
    "        if self.grayscale:\n",
    "            img = img.convert(\"L\")\n",
    "        else:\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        img = img.resize((self.img_w, self.img_h), resample=Image.BILINEAR)\n",
    "        arr = np.array(img, dtype=np.float32)\n",
    "\n",
    "        if self.grayscale:\n",
    "            # (H, W) -> (1, H, W)\n",
    "            tensor = torch.from_numpy(arr).unsqueeze(0)\n",
    "        else:\n",
    "            # (H, W, C) -> (C, H, W)\n",
    "            tensor = torch.from_numpy(arr).permute(2, 0, 1)\n",
    "\n",
    "        if self.normalize:\n",
    "            tensor = tensor / 255.0\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        rec = self.records[idx]\n",
    "        img_tensor = self._load_image_tensor(rec)\n",
    "\n",
    "        if self.return_line:\n",
    "            return img_tensor, int(rec[\"line\"])\n",
    "        return img_tensor\n",
    "\n",
    "\n",
    "normalize = bool(config[\"data\"].get(\"normalize\", True))\n",
    "grayscale = bool(config[\"data\"].get(\"grayscale\", True))\n",
    "\n",
    "full_dataset = TrajectoryFrameDataset(\n",
    "    frame_records=frame_records,\n",
    "    img_width=img_w,\n",
    "    img_height=img_h,\n",
    "    in_channels=in_channels,\n",
    "    grayscale=grayscale,\n",
    "    normalize=normalize,\n",
    "    return_line=line_col_present,\n",
    ")\n",
    "\n",
    "print(\"Dataset length:\", len(full_dataset))\n",
    "\n",
    "# Train/validation split\n",
    "train_fraction = float(config[\"data\"].get(\"train_fraction\", 0.9))\n",
    "train_fraction = max(0.0, min(1.0, train_fraction))\n",
    "n_total = len(full_dataset)\n",
    "n_train = int(round(train_fraction * n_total))\n",
    "n_val = n_total - n_train\n",
    "\n",
    "if n_train == 0 or n_val == 0:\n",
    "    # Degenerate cases: all train or all validation\n",
    "    train_dataset = full_dataset\n",
    "    val_dataset: Optional[Dataset] = None\n",
    "    print(\"No train/validation split performed.\")\n",
    "else:\n",
    "    generator = torch.Generator().manual_seed(config.get(\"seed\", 0))\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset,\n",
    "        [n_train, n_val],\n",
    "        generator=generator,\n",
    "    )\n",
    "    print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "train_cfg = config[\"training\"]\n",
    "batch_size = int(train_cfg[\"batch_size\"])\n",
    "\n",
    "# IMPORTANT: start with num_workers = 0 to avoid huge spawn/pickling overhead on big datasets\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = None\n",
    "if val_dataset is not None:\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "# Optional quick sanity check: this will only load ONE batch worth of images\n",
    "sample_batch = next(iter(train_loader))\n",
    "if line_col_present:\n",
    "    sample_imgs, sample_lines = sample_batch\n",
    "else:\n",
    "    sample_imgs = sample_batch\n",
    "print(\"Sample batch image tensor shape:\", sample_imgs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e5b8",
   "metadata": {},
   "source": [
    "## 5. CNN Autoencoder definition (2D latent space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80204937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional autoencoder with a configurable stack of encoder conv blocks\n",
    "    and mirrored decoder transposed-conv blocks. Latent space is a vector of size\n",
    "    `latent_dim` (here typically 2).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        img_height: int,\n",
    "        img_width: int,\n",
    "        conv_channels: List[int],\n",
    "        kernel_size: int = 3,\n",
    "        latent_dim: int = 2,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.img_h = img_height\n",
    "        self.img_w = img_width\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers: List[nn.Module] = []\n",
    "        c_in = in_channels\n",
    "        for c_out in conv_channels:\n",
    "            encoder_layers.append(\n",
    "                nn.Conv2d(\n",
    "                    c_in,\n",
    "                    c_out,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=2,  # downsample by factor 2\n",
    "                    padding=padding,\n",
    "                    bias=False,\n",
    "                )\n",
    "            )\n",
    "            encoder_layers.append(nn.BatchNorm2d(c_out))\n",
    "            encoder_layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout > 0.0:\n",
    "                encoder_layers.append(nn.Dropout2d(p=dropout))\n",
    "            c_in = c_out\n",
    "        self.encoder_conv = nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Determine shape after encoder convs using a dummy pass\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, img_height, img_width)\n",
    "            enc_out = self.encoder_conv(dummy)\n",
    "        self.enc_out_shape = enc_out.shape[1:]  # (C, H, W)\n",
    "        enc_flat_dim = int(enc_out.numel() // enc_out.shape[0])\n",
    "\n",
    "        # Linear layers for latent code\n",
    "        self.fc_enc = nn.Linear(enc_flat_dim, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, enc_flat_dim)\n",
    "\n",
    "        # Decoder: mirror conv stacks with ConvTranspose2d\n",
    "        decoder_layers: List[nn.Module] = []\n",
    "        rev_channels = list(reversed(conv_channels))\n",
    "        for i, c_in_dec in enumerate(rev_channels):\n",
    "            if i + 1 < len(rev_channels):\n",
    "                c_out_dec = rev_channels[i + 1]\n",
    "                final = False\n",
    "            else:\n",
    "                c_out_dec = in_channels\n",
    "                final = True\n",
    "\n",
    "            # stride=2, padding=1, output_padding=1 yields exact doubling in size\n",
    "            decoder_layers.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    c_in_dec,\n",
    "                    c_out_dec,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=2,\n",
    "                    padding=padding,\n",
    "                    output_padding=1,\n",
    "                    bias=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if not final:\n",
    "                decoder_layers.append(nn.BatchNorm2d(c_out_dec))\n",
    "                decoder_layers.append(nn.ReLU(inplace=True))\n",
    "            else:\n",
    "                # Map back to [0,1] for normalized intensities\n",
    "                decoder_layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(*decoder_layers)\n",
    "\n",
    "        # Sanity check: confirm output shape matches input shape\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, in_channels, img_height, img_width)\n",
    "            recon = self.forward(x)[0]\n",
    "        if recon.shape != x.shape:\n",
    "            raise RuntimeError(\n",
    "                f\"Autoencoder output shape {recon.shape} does not match input shape {x.shape}. \"\n",
    "                f\"Adjust conv configuration.\"\n",
    "            )\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.encoder_conv(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        z = self.fc_enc(h)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.fc_dec(z)\n",
    "        h = h.view(z.size(0), *self.enc_out_shape)\n",
    "        x_recon = self.decoder_conv(h)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, z\n",
    "\n",
    "\n",
    "conv_channels = [int(c) for c in model_cfg[\"conv_channels\"]]\n",
    "kernel_size = int(model_cfg.get(\"kernel_size\", 3))\n",
    "latent_dim = int(model_cfg.get(\"latent_dim\", 2))\n",
    "dropout = float(model_cfg.get(\"dropout\", 0.0))\n",
    "\n",
    "ae_model = ConvAutoencoder(\n",
    "    in_channels=in_channels,\n",
    "    img_height=img_h,\n",
    "    img_width=img_w,\n",
    "    conv_channels=conv_channels,\n",
    "    kernel_size=kernel_size,\n",
    "    latent_dim=latent_dim,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "print(ae_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7ce94",
   "metadata": {},
   "source": [
    "## 6. Training setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9089ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "device_cfg = config.get(\"device\", {\"type\": \"auto\"})\n",
    "device_type = device_cfg.get(\"type\", \"auto\")\n",
    "\n",
    "if device_type == \"cuda\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "elif device_type == \"cpu\":\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Training device:\", device)\n",
    "\n",
    "ae_model = ae_model.to(device)\n",
    "\n",
    "train_cfg = config[\"training\"]\n",
    "num_epochs = int(train_cfg[\"num_epochs\"])\n",
    "learning_rate = float(train_cfg[\"learning_rate\"])\n",
    "weight_decay = float(train_cfg.get(\"weight_decay\", 0.0))\n",
    "optimizer_name = train_cfg.get(\"optimizer\", \"adam\").lower()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if optimizer_name == \"adam\":\n",
    "    optimizer = torch.optim.Adam(\n",
    "        ae_model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "elif optimizer_name == \"sgd\":\n",
    "    optimizer = torch.optim.SGD(\n",
    "        ae_model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        momentum=0.9,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "print(\"Optimizer:\", optimizer_name)\n",
    "print(\"Learning rate:\", learning_rate)\n",
    "print(\"Weight decay:\", weight_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f0c942",
   "metadata": {},
   "source": [
    "## 7. Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9160f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> float:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # tqdm wrapper here\n",
    "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "\n",
    "    for batch in pbar:\n",
    "        if line_col_present:\n",
    "            imgs, _lines = batch\n",
    "        else:\n",
    "            imgs = batch\n",
    "\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon, _ = model(imgs)\n",
    "        loss = criterion(recon, imgs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = imgs.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        # live update inside progress bar\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return running_loss / max(1, total_samples)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # tqdm wrapper here\n",
    "    pbar = tqdm(loader, desc=\"Eval\", leave=False)\n",
    "\n",
    "    for batch in pbar:\n",
    "        if line_col_present:\n",
    "            imgs, _lines = batch\n",
    "        else:\n",
    "            imgs = batch\n",
    "\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        recon, _ = model(imgs)\n",
    "        loss = criterion(recon, imgs)\n",
    "\n",
    "        batch_size = imgs.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    return running_loss / max(1, total_samples)\n",
    "\n",
    "\n",
    "history = {\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "    train_loss = train_one_epoch(ae_model, train_loader, optimizer, device)\n",
    "\n",
    "    if val_loader is not None:\n",
    "        val_loss = eval_one_epoch(ae_model, val_loader, device)\n",
    "    else:\n",
    "        val_loss = float(\"nan\")\n",
    "\n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "    print(f\"train_loss={train_loss:.6f}, val_loss={val_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fa72b",
   "metadata": {},
   "source": [
    "## 8. Plot training curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.head()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history_df[\"epoch\"], history_df[\"train_loss\"], label=\"train\")\n",
    "if not np.all(np.isnan(history_df[\"val_loss\"].to_numpy())):\n",
    "    plt.plot(history_df[\"epoch\"], history_df[\"val_loss\"], label=\"val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE reconstruction loss\")\n",
    "plt.title(\"Autoencoder training curves\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154fe68b",
   "metadata": {},
   "source": [
    "## 9. Save model, config snapshot, and training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6fa6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "logging_cfg = config.get(\"logging\", {})\n",
    "output_dir = logging_cfg.get(\"output_dir\", \"autoencoder_runs\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "data_cfg = config[\"data\"]\n",
    "folder = data_cfg[\"folder\"]\n",
    "folder_name = os.path.basename(os.path.abspath(folder)) or \"root\"\n",
    "\n",
    "use_all_times = bool(data_cfg.get(\"use_all_times\", False))\n",
    "if use_all_times:\n",
    "    tw_str = \"all\"\n",
    "else:\n",
    "    start_time = data_cfg.get(\"start_time\", \"00:00:00\")\n",
    "    end_time = data_cfg.get(\"end_time\", \"23:59:59\")\n",
    "    tw_str = f\"{start_time.replace(':','')}-{end_time.replace(':','')}\"\n",
    "\n",
    "mode = \"gray\" if data_cfg.get(\"grayscale\", True) else \"rgb\"\n",
    "frame_step = int(data_cfg.get(\"frame_step\", 1))\n",
    "\n",
    "base_data_id = f\"{folder_name}_w{img_w}h{img_h}_{mode}_tw{tw_str}_step{frame_step}\"\n",
    "\n",
    "config_base = os.path.splitext(os.path.basename(CONFIG_PATH))[0]\n",
    "seed_value = config.get(\"seed\", 0)\n",
    "\n",
    "run_id = f\"{base_data_id}_{config_base}_seed{seed_value}\"\n",
    "\n",
    "model_base_name = f\"ae_{run_id}\"\n",
    "model_path = os.path.join(output_dir, model_base_name + \".pt\")\n",
    "history_path = os.path.join(output_dir, model_base_name + \"_history.csv\")\n",
    "config_snapshot_path = os.path.join(output_dir, model_base_name + \"_config.yaml\")\n",
    "\n",
    "# Save state dict with some metadata\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": ae_model.state_dict(),\n",
    "        \"config\": config,\n",
    "        \"history\": history,\n",
    "        \"data_folder\": folder,\n",
    "        \"frame_step\": frame_step,\n",
    "        \"time_window\": {\n",
    "            \"use_all_times\": use_all_times,\n",
    "            \"start_time\": data_cfg.get(\"start_time\", \"00:00:00\"),\n",
    "            \"end_time\": data_cfg.get(\"end_time\", \"23:59:59\"),\n",
    "        },\n",
    "    },\n",
    "    model_path,\n",
    ")\n",
    "print(\"Saved model to:\", model_path)\n",
    "\n",
    "# Save training history\n",
    "history_df.to_csv(history_path, index=False)\n",
    "print(\"Saved training history to:\", history_path)\n",
    "\n",
    "# Save a copy of the used config with a name that matches the model\n",
    "with open(config_snapshot_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(config, f)\n",
    "print(\"Saved config snapshot to:\", config_snapshot_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f4ddf",
   "metadata": {},
   "source": [
    "## 10. Reconstruction examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0059e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "@torch.no_grad()\n",
    "def plot_reconstructions(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    num_examples: int = 8,\n",
    "    out_path: Optional[str] = None,\n",
    "):\n",
    "    model.eval()\n",
    "    # Fetch first batch\n",
    "    batch = next(iter(loader))\n",
    "    if line_col_present:\n",
    "        imgs, _lines = batch\n",
    "    else:\n",
    "        imgs = batch\n",
    "\n",
    "    imgs = imgs.to(device)\n",
    "    recon, _ = model(imgs)\n",
    "\n",
    "    imgs = imgs.cpu().numpy()\n",
    "    recon = recon.cpu().numpy()\n",
    "\n",
    "    n = min(num_examples, imgs.shape[0])\n",
    "\n",
    "    fig, axes = plt.subplots(2, n, figsize=(n * 2, 4))\n",
    "    for i in range(n):\n",
    "        orig = imgs[i]\n",
    "        rec = recon[i]\n",
    "\n",
    "        if in_channels == 1:\n",
    "            orig_img = orig[0]\n",
    "            rec_img = rec[0]\n",
    "            cmap = \"gray\"\n",
    "        else:\n",
    "            orig_img = np.transpose(orig, (1, 2, 0))\n",
    "            rec_img = np.transpose(rec, (1, 2, 0))\n",
    "            cmap = None\n",
    "\n",
    "        axes[0, i].imshow(orig_img, cmap=cmap, vmin=0.0, vmax=1.0)\n",
    "        axes[0, i].axis(\"off\")\n",
    "        axes[0, i].set_title(\"orig\")\n",
    "\n",
    "        axes[1, i].imshow(rec_img, cmap=cmap, vmin=0.0, vmax=1.0)\n",
    "        axes[1, i].axis(\"off\")\n",
    "        axes[1, i].set_title(\"recon\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if out_path is not None:\n",
    "        fig.savefig(out_path, dpi=300)\n",
    "        print(\"Saved reconstruction grid to:\", out_path)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "num_recon_examples = int(logging_cfg.get(\"num_recon_examples\", 8))\n",
    "recon_out_path = os.path.join(output_dir, model_base_name + \"_recon.png\")\n",
    "\n",
    "loader_for_recon = val_loader if val_loader is not None else train_loader\n",
    "plot_reconstructions(\n",
    "    ae_model,\n",
    "    loader_for_recon,\n",
    "    device,\n",
    "    num_examples=num_recon_examples,\n",
    "    out_path=recon_out_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b2bc9",
   "metadata": {},
   "source": [
    "## 11. Latent space scatterplot (2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "@torch.no_grad()\n",
    "def compute_latent_and_lines(\n",
    "    model: nn.Module,\n",
    "    dataset: Dataset,\n",
    "    device: torch.device,\n",
    "    batch_size: int = 1024,\n",
    ") -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    zs = []\n",
    "    ls = [] if line_col_present else None\n",
    "\n",
    "    for batch in loader:\n",
    "        if line_col_present:\n",
    "            imgs, batch_lines = batch\n",
    "            ls.append(batch_lines.numpy())\n",
    "        else:\n",
    "            imgs = batch\n",
    "\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        _, z = model(imgs)\n",
    "        zs.append(z.cpu().numpy())\n",
    "\n",
    "    Z = np.concatenate(zs, axis=0)\n",
    "    if line_col_present and ls is not None:\n",
    "        lines = np.concatenate(ls, axis=0)\n",
    "    else:\n",
    "        lines = None\n",
    "    return Z, lines\n",
    "\n",
    "\n",
    "Z, latent_lines = compute_latent_and_lines(\n",
    "    ae_model, full_dataset, device, batch_size=1024\n",
    ")\n",
    "print(\"Latent shape:\", Z.shape)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "if Z.shape[1] >= 2:\n",
    "    x = Z[:, 0]\n",
    "    y = Z[:, 1]\n",
    "else:\n",
    "    x = Z[:, 0]\n",
    "    y = np.zeros_like(x)\n",
    "\n",
    "if latent_lines is not None:\n",
    "    # Color by day/line (same semantics as the first notebook's UMAP plots)\n",
    "    cat = pd.Categorical(latent_lines)\n",
    "    codes = cat.codes\n",
    "    sc = plt.scatter(x, y, c=codes, s=3, alpha=0.7, cmap=\"tab20\")\n",
    "else:\n",
    "    sc = plt.scatter(x, y, s=3, alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"latent dim 1\")\n",
    "plt.ylabel(\"latent dim 2\" if Z.shape[1] >= 2 else \"0\")\n",
    "plt.title(\"Autoencoder latent space (2D)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "latent_out_path = os.path.join(output_dir, model_base_name + \"_latent_scatter.png\")\n",
    "plt.savefig(latent_out_path, dpi=300)\n",
    "print(\"Saved latent scatterplot to:\", latent_out_path)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
